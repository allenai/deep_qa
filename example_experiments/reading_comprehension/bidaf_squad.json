{
    "model_class": "BidirectionalAttentionFlow",
    "model_serialization_prefix": "/net/efs/aristo/dlfa/models/bidaf",
    "encoder": {
        "word": {
          "type": "cnn",
          "ngram_filter_sizes": [5],
          "num_filters": 100
        }
    },
    "pretrained_embeddings_file": "/net/efs/aristo/dlfa/glove/glove.6B.100d.txt.gz",
    "project_embeddings": false,
    "fine_tune_embeddings": false,
    "seq2seq_encoder": {
        "default": {
            "type": "bi_gru",
            "encoder_params": {
                "units": 100
            },
            "wrapper_params": {}
        }
    },
    "data_generator": {
      "dynamic_padding": true,
      "adaptive_batch_sizes": true,
      "adaptive_memory_usage_constant": 440000,
      "maximum_batch_size": 60
    },
    // This is not quite the same as Min's paper; we don't have encoder dropout yet.
    "embedding_dropout": 0.2,
    "patience": 3,
    "embedding_dim": {"words": 100, "characters": 8},
    "num_epochs": 20,
    "optimizer": {
      "type": "adadelta",
      "lr": 0.5
    },
    "validation_files": ["/net/efs/aristo/dlfa/squad/processed/dev.tsv"],
    "train_files": ["/net/efs/aristo/dlfa/squad/processed/train.tsv"]
}
